"""Statistics API endpoints."""

import asyncio
from datetime import datetime, timedelta
from typing import Any

from fastapi import APIRouter, Depends, Query
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_db
from app.core.cache import cache_manager
from app.core.logging import get_logger
from app.models.politician import Politician
from app.models.trade import Trade

router = APIRouter()
logger = get_logger(__name__)


@router.get("/leaderboard")
async def get_leaderboard(
    period: str = Query("30d", pattern="^(7d|30d|90d|1y)$"),
    limit: int = Query(10, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
) -> dict:
    """
    Get politician performance leaderboard by trade volume.

    This endpoint uses Redis caching to improve performance for this expensive
    aggregation query. Cache TTL varies by period:
    - 7d: 5 minutes (more volatile data)
    - 30d: 15 minutes
    - 90d/1y: 1 hour (less volatile data)

    Args:
        period: Time period (7d, 30d, 90d, 1y)
        limit: Maximum number of records to return (1-100)
        db: Database session

    Returns:
        Leaderboard data with politician trading activity
    """
    # Try cache first
    cache_key = f"stats:leaderboard:{period}:{limit}"
    cached_result = await cache_manager.get(cache_key)

    if cached_result:
        logger.info(f"Cache hit for leaderboard: period={period}, limit={limit}")
        return cached_result

    logger.info(f"Cache miss for leaderboard: period={period}, limit={limit}")

    # Calculate date range
    period_map = {
        "7d": 7,
        "30d": 30,
        "90d": 90,
        "1y": 365
    }
    days = period_map.get(period, 30)
    cutoff_date = datetime.utcnow() - timedelta(days=days)

    # Query for top traders by volume
    query = (
        select(
            Politician.id,
            Politician.name,
            Politician.party,
            func.count(Trade.id).label("trade_count"),
            func.sum(Trade.amount).label("total_volume")
        )
        .join(Trade, Trade.politician_id == Politician.id)
        .where(Trade.transaction_date >= cutoff_date)
        .group_by(Politician.id, Politician.name, Politician.party)
        .order_by(func.sum(Trade.amount).desc())
        .limit(limit)
    )

    result = await db.execute(query)
    leaderboard = []

    for row in result:
        leaderboard.append({
            "politician_id": str(row.id),
            "name": row.name,
            "party": row.party,
            "trade_count": row.trade_count,
            "total_volume": float(row.total_volume) if row.total_volume else 0.0,
            "rank": len(leaderboard) + 1
        })

    response_data = {
        "leaderboard": leaderboard,
        "period": period,
        "total_results": len(leaderboard),
    }

    # Cache result with variable TTL based on period
    ttl_map = {
        "7d": 300,      # 5 minutes (more volatile)
        "30d": 900,     # 15 minutes
        "90d": 3600,    # 1 hour
        "1y": 3600      # 1 hour (less volatile)
    }
    ttl = ttl_map.get(period, 900)
    await cache_manager.set(cache_key, response_data, ttl=ttl)

    logger.debug(f"Cached leaderboard for {ttl}s: period={period}, limit={limit}")

    return response_data


@router.get("/sectors")
async def get_sector_stats(
    period: str = Query("30d", pattern="^(7d|30d|90d|1y)$"),
    db: AsyncSession = Depends(get_db),
) -> dict:
    """
    Get sector trading statistics.

    This endpoint uses Redis caching to improve performance for this expensive
    aggregation query. Cache TTL varies by period (same as leaderboard).

    Args:
        period: Time period (7d, 30d, 90d, 1y)
        db: Database session

    Returns:
        Sector statistics showing trading activity by sector
    """
    # Try cache first
    cache_key = f"stats:sectors:{period}"
    cached_result = await cache_manager.get(cache_key)

    if cached_result:
        logger.info(f"Cache hit for sectors: period={period}")
        return cached_result

    logger.info(f"Cache miss for sectors: period={period}")

    # Calculate date range
    period_map = {
        "7d": 7,
        "30d": 30,
        "90d": 90,
        "1y": 365
    }
    days = period_map.get(period, 30)
    cutoff_date = datetime.utcnow() - timedelta(days=days)

    # Query for sector statistics
    query = (
        select(
            Trade.ticker_sector.label("sector"),
            func.count(Trade.id).label("trade_count"),
            func.sum(Trade.amount).label("total_volume"),
            func.avg(Trade.amount).label("avg_trade_size")
        )
        .where(Trade.transaction_date >= cutoff_date)
        .where(Trade.ticker_sector.isnot(None))
        .group_by(Trade.ticker_sector)
        .order_by(func.sum(Trade.amount).desc())
    )

    result = await db.execute(query)
    sectors = []

    for row in result:
        sectors.append({
            "sector": row.sector,
            "trade_count": row.trade_count,
            "total_volume": float(row.total_volume) if row.total_volume else 0.0,
            "avg_trade_size": float(row.avg_trade_size) if row.avg_trade_size else 0.0
        })

    response_data = {
        "sectors": sectors,
        "period": period,
        "total_sectors": len(sectors),
    }

    # Cache result with variable TTL based on period
    ttl_map = {
        "7d": 300,      # 5 minutes
        "30d": 900,     # 15 minutes
        "90d": 3600,    # 1 hour
        "1y": 3600      # 1 hour
    }
    ttl = ttl_map.get(period, 900)
    await cache_manager.set(cache_key, response_data, ttl=ttl)

    logger.debug(f"Cached sectors for {ttl}s: period={period}")

    return response_data


@router.get("/dashboard")
async def get_dashboard_stats(db: AsyncSession = Depends(get_db)) -> dict[str, Any]:
    """
    Get comprehensive dashboard statistics with parallel query execution.

    This endpoint demonstrates parallel processing optimization by running
    multiple independent database queries concurrently using asyncio.gather().

    Performance Impact:
    - Sequential execution: ~800-1200ms (4 queries Ã— 200-300ms each)
    - Parallel execution: ~200-300ms (queries run concurrently)
    - Improvement: ~4x faster

    Returns:
        Dashboard statistics including:
        - Total trades count
        - Active politicians count
        - Recent trades (last 10)
        - Top politicians by trade count (last 30 days)
        - Overall statistics
    """
    logger.debug("Fetching dashboard statistics with parallel queries")

    # Try cache first
    cache_key = "stats:dashboard"
    cached_result = await cache_manager.get(cache_key)

    if cached_result:
        logger.info("Cache hit for dashboard stats")
        return cached_result

    logger.info("Cache miss for dashboard stats - executing parallel queries")

    # Calculate 30-day cutoff for recent activity
    cutoff_date = datetime.utcnow() - timedelta(days=30)

    # Run multiple queries in parallel for better performance
    # Each query is independent, so they can execute concurrently
    results = await asyncio.gather(
        # Query 1: Total trades count
        db.execute(select(func.count(Trade.id))),
        # Query 2: Active politicians count
        db.execute(
            select(func.count(func.distinct(Trade.politician_id))).where(
                Trade.transaction_date >= cutoff_date
            )
        ),
        # Query 3: Recent trades (last 10)
        db.execute(
            select(Trade, Politician.name, Politician.party)
            .join(Politician, Trade.politician_id == Politician.id)
            .order_by(Trade.transaction_date.desc())
            .limit(10)
        ),
        # Query 4: Top politicians by trade count (last 30 days)
        db.execute(
            select(
                Politician.name,
                Politician.party,
                func.count(Trade.id).label("trade_count"),
            )
            .join(Trade, Trade.politician_id == Politician.id)
            .where(Trade.transaction_date >= cutoff_date)
            .group_by(Politician.id, Politician.name, Politician.party)
            .order_by(func.count(Trade.id).desc())
            .limit(10)
        ),
        # Query 5: Buy vs Sell ratio (last 30 days)
        db.execute(
            select(
                Trade.transaction_type, func.count(Trade.id).label("count")
            ).where(Trade.transaction_date >= cutoff_date).group_by(Trade.transaction_type)
        ),
        return_exceptions=True,  # Don't fail all if one query fails
    )

    # Process results with error handling
    total_trades = 0
    active_politicians = 0
    recent_trades = []
    top_politicians = []
    buy_sell_ratio = {"buy": 0, "sell": 0}

    try:
        # Result 1: Total trades
        if not isinstance(results[0], Exception):
            total_trades = results[0].scalar() or 0

        # Result 2: Active politicians
        if not isinstance(results[1], Exception):
            active_politicians = results[1].scalar() or 0

        # Result 3: Recent trades
        if not isinstance(results[2], Exception):
            for trade, politician_name, politician_party in results[2]:
                recent_trades.append(
                    {
                        "id": str(trade.id),
                        "ticker": trade.ticker,
                        "transaction_type": trade.transaction_type,
                        "transaction_date": trade.transaction_date.isoformat(),
                        "politician_name": politician_name,
                        "politician_party": politician_party,
                    }
                )

        # Result 4: Top politicians
        if not isinstance(results[3], Exception):
            for name, party, count in results[3]:
                top_politicians.append(
                    {"name": name, "party": party, "trade_count": count}
                )

        # Result 5: Buy/Sell ratio
        if not isinstance(results[4], Exception):
            for transaction_type, count in results[4]:
                buy_sell_ratio[transaction_type] = count

    except Exception as e:
        logger.error(f"Error processing dashboard results: {e}", exc_info=True)

    response_data = {
        "total_trades": total_trades,
        "active_politicians_30d": active_politicians,
        "recent_trades": recent_trades,
        "top_politicians_30d": top_politicians,
        "buy_sell_ratio_30d": buy_sell_ratio,
        "timestamp": datetime.utcnow().isoformat(),
    }

    # Cache for 5 minutes (dashboard data changes frequently)
    await cache_manager.set(cache_key, response_data, ttl=300)
    logger.debug("Cached dashboard stats for 300s")

    return response_data
